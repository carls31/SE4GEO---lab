{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://discomap.eea.europa.eu/map/fme/latest/files.txt'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from se4g_helper import download_request\n",
    "# website containing countries & pollutants list\n",
    "'''https://discomap.eea.europa.eu/map/fme/latest/files.txt'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set countries to be downloaded\n",
    "COUNTRIES = ['AD','AT','BA','BE','BG','CH','CY','CZ','DE','DK','EE','ES','FI','SE']\n",
    "# Set pollutant to be downloaded\n",
    "POLLUTANTS = ['SO2','CO','O3','PM25','PM10']\n",
    "#CY_PM10\n",
    "#FI_CO.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and get the dataframe file name\n",
    "dir = download_request(COUNTRIES, POLLUTANTS)\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the folder where the dataframe file is located\n",
    "folder_out = 'data'\n",
    "full_file = os.path.join(folder_out, 'AD_SO2.csv')\n",
    "# Read the file\n",
    "df = pd.read_csv(full_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['station_code', \n",
    "              'station_name', \n",
    "              'station_altitude', \n",
    "              'network_countrycode', \n",
    "              'pollutant', \n",
    "              'value_datetime_begin',\n",
    "              'value_datetime_end',\n",
    "              'value_datetime_updated',\n",
    "              'value_numeric']\n",
    "df[df_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prova = 'FI_CO_NONFUNZIA'\n",
    "if file_prova != 'FI_CO.csv' and file_prova != 'FI_O_NONFUNZIA':\n",
    "    print('Ã¨ ok')\n",
    "else:\n",
    "    print('non funziona')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for country in COUNTRIES:\n",
    "    for pollutant in POLLUTANTS:\n",
    "        \n",
    "        fileName = \"%s_%s.csv\" % (country, pollutant)\n",
    "        if fileName != \"FI_CO.csv\" and fileName != \"CY_PM10.csv\":\n",
    "            df_temp = pd.read_csv(\"data/\"+fileName)\n",
    "            #data = df_temp[df_columns]\n",
    "            dfs.append(df_temp[df_columns])\n",
    "            #df_all = pd.DataFrame(data)\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv(\"data/\"+fileName)\n",
    "'''print(os.path.isfile(\"data/\"+fileName))\n",
    "print('fileName', fileName)\n",
    "dfs'''\n",
    "pd.read_csv(\"data/CY_PM10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se4g_helper import login_required\n",
    "\n",
    "print(\"Welcome to the application!\")\n",
    "print(\"Please log in to continue.\")\n",
    "con = login_required()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "print(datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\"))\n",
    "folder_out = 'folderout'\n",
    "fileName = 'filename'\n",
    "os.path.join(folder_out,str(datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")), fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "folder_out = 'prova_directory'\n",
    "dir = datetime.now().strftime(\"%d-%m-%Y_%H_%M_%S\")\n",
    "if not os.path.exists(os.path.join(folder_out, dir)):\n",
    "    os.mkdir(folder_out)\n",
    "    os.mkdir(os.path.join(folder_out, dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
<<<<<<< HEAD
      "22-05-2023_17_35_26 directory created\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_CO.csv\n",
      "Saved locally as: AT_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_O3.csv\n",
      "Saved locally as: AT_O3.csv \n",
      "-----\n",
      "Download finished\n",
      "AT_CO.csv exist\n",
      "AT_O3.csv exist\n",
      "Database assembled\n",
      "Dataset created successfully\n"
     ]
    }
   ],
   "source": [
    "from se4g_helper import download_request, build_dataframe, update_dataset\n",
    "\n",
    "# Download and get the dataframe file name\n",
    "dir = download_request(COUNTRIES = ['AT'], POLLUTANTS = ['CO','O3'], folder_out = 'data_prova')\n",
    "\n",
    "# Build the dataframe with the required structure\n",
    "df = build_dataframe(dir, COUNTRIES = ['AT'], POLLUTANTS = ['CO','O3'], folder_out = 'data_prova')\n",
    "\n",
    "# Update the final dataset\n",
    "update_dataset(df, folder_out = 'data_prova')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "22-05-2023_18_13_58 directory created\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AD_SO2.csv\n",
      "Saved locally as: AD_SO2.csv \n",
      "-----\n",
=======
      "22-05-2023_15_50_18 directory created\n",
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AD_CO.csv\n",
      "Saved locally as: AD_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AD_O3.csv\n",
      "Saved locally as: AD_O3.csv \n",
      "-----\n",
<<<<<<< HEAD
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AD_PM10.csv\n",
      "Saved locally as: AD_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_SO2.csv\n",
      "Saved locally as: AT_SO2.csv \n",
      "-----\n",
=======
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_CO.csv\n",
      "Saved locally as: AT_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_O3.csv\n",
      "Saved locally as: AT_O3.csv \n",
      "-----\n",
<<<<<<< HEAD
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/AT_PM10.csv\n",
      "Saved locally as: AT_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BA_SO2.csv\n",
      "Saved locally as: BA_SO2.csv \n",
      "-----\n",
=======
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BA_CO.csv\n",
      "Saved locally as: BA_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BA_O3.csv\n",
      "Saved locally as: BA_O3.csv \n",
      "-----\n",
<<<<<<< HEAD
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BA_PM10.csv\n",
      "Saved locally as: BA_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BE_SO2.csv\n",
      "Saved locally as: BE_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BE_CO.csv\n",
      "Saved locally as: BE_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BE_O3.csv\n",
      "Saved locally as: BE_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BE_PM10.csv\n",
      "Saved locally as: BE_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BG_SO2.csv\n",
      "Saved locally as: BG_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BG_CO.csv\n",
      "Saved locally as: BG_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BG_O3.csv\n",
      "Saved locally as: BG_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/BG_PM10.csv\n",
      "Saved locally as: BG_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CH_SO2.csv\n",
      "Saved locally as: CH_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CH_CO.csv\n",
      "Saved locally as: CH_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CH_O3.csv\n",
      "Saved locally as: CH_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CH_PM10.csv\n",
      "Saved locally as: CH_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CY_SO2.csv\n",
      "Saved locally as: CY_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CY_CO.csv\n",
      "Saved locally as: CY_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CY_O3.csv\n",
      "Saved locally as: CY_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CY_PM10.csv\n",
      "Saved locally as: CY_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CZ_SO2.csv\n",
      "Saved locally as: CZ_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CZ_CO.csv\n",
      "Saved locally as: CZ_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CZ_O3.csv\n",
      "Saved locally as: CZ_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/CZ_PM10.csv\n",
      "Saved locally as: CZ_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DE_SO2.csv\n",
      "Saved locally as: DE_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DE_CO.csv\n",
      "Saved locally as: DE_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DE_O3.csv\n",
      "Saved locally as: DE_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DE_PM10.csv\n",
      "Saved locally as: DE_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DK_SO2.csv\n",
      "Saved locally as: DK_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DK_CO.csv\n",
      "Saved locally as: DK_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DK_O3.csv\n",
      "Saved locally as: DK_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/DK_PM10.csv\n",
      "Saved locally as: DK_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/EE_SO2.csv\n",
      "Saved locally as: EE_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/EE_CO.csv\n",
      "Saved locally as: EE_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/EE_O3.csv\n",
      "Saved locally as: EE_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/EE_PM10.csv\n",
      "Saved locally as: EE_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/ES_SO2.csv\n",
      "Saved locally as: ES_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/ES_CO.csv\n",
      "Saved locally as: ES_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/ES_O3.csv\n",
      "Saved locally as: ES_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/ES_PM10.csv\n",
      "Saved locally as: ES_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/FI_SO2.csv\n",
      "Saved locally as: FI_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/FI_CO.csv\n",
      "Saved locally as: FI_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/FI_O3.csv\n",
      "Saved locally as: FI_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/FI_PM10.csv\n",
      "Saved locally as: FI_PM10.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/SE_SO2.csv\n",
      "Saved locally as: SE_SO2.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/SE_CO.csv\n",
      "Saved locally as: SE_CO.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/SE_O3.csv\n",
      "Saved locally as: SE_O3.csv \n",
      "-----\n",
      "Downloading: http://discomap.eea.europa.eu/map/fme/latest/SE_PM10.csv\n",
      "Saved locally as: SE_PM10.csv \n",
      "-----\n",
      "Download finished\n",
      "Database assembled\n",
      "Dataset updated and saved successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'se4g_pollution_dataset.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
=======
      "Download finished\n"
     ]
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
    }
   ],
   "source": [
    "from se4g_helper import download_request, build_dataframe, update_dataset\n",
    "\n",
    "# Download and get the dataframe file name\n",
<<<<<<< HEAD
    "dir = download_request(folder_out = 'data_prova')\n",
    "\n",
    "# Build the dataframe with the required structure\n",
    "df = build_dataframe(dir, folder_out = 'data_prova')\n",
    "\n",
    "# Update the final dataset \n",
    "update_dataset(df, folder_out = 'data_prova')\n",
    "\n"
=======
    "dir = download_request(COUNTRIES = ['AD','AT','BA'], POLLUTANTS = ['CO','O3'], folder_out = 'data_prova')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database assembled\n"
     ]
    }
   ],
   "source": [
    "# Build the dataframe with the required structure\n",
    "df = build_dataframe(dir, COUNTRIES = ['AD','AT','BA'], POLLUTANTS = ['CO','O3'], folder_out = 'data_prova')"
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network_countrycode,network_localid,network_name,network_namespace,network_timezone,pollutant,samplingpoint_localid,samplingpoint_namespace,samplingpoint_x,samplingpoint_y,coordsys,station_code,station_localid,station_name,station_namespace,value_datetime_begin,value_datetime_end,value_datetime_inserted,value_datetime_updated,value_numeric,value_validity,value_verification,station_altitude,value_unit\n"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_prova\\\\se4g_pollution_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Update the final dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m update_dataset(df, folder_out \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mdata_prova\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\Documents\\GitHub\\SE4GEO-Lab\\se4g_helper.py:83\u001b[0m, in \u001b[0;36mupdate_dataset\u001b[1;34m(new_df, folder_out)\u001b[0m\n\u001b[0;32m     80\u001b[0m full_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_out, fileName)\n\u001b[0;32m     82\u001b[0m \u001b[39m# Open the CSV dataset\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(full_path)\n\u001b[0;32m     85\u001b[0m \u001b[39m# Update the dataset by adding some data\u001b[39;00m\n\u001b[0;32m     86\u001b[0m updated_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, new_df], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Utente\\mambaforge\\envs\\se4g\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_prova\\\\se4g_pollution_dataset.csv'"
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Define the CSV file path\n",
    "file_path = 'data_prova/22-05-2023_16_32_14/ES_PM10.csv'\n",
    "\n",
    "# Check if the first row starts with the specific string\n",
    "with open(file_path, 'r') as file:\n",
    "    first_line = file.readline().strip()\n",
    "if first_line.startswith('network_countrycode'):\n",
    "    print('file ok')\n",
    "else:\n",
    "    print(first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_file_path = 'data_prova/22-05-2023_16_32_14/ES_PM25.csv'  \n",
    "\n",
    "try:\n",
    "    with open(html_file_path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "        print(html_content)\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content type: HTML\n"
     ]
    }
   ],
   "source": [
    "def check_file_content(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        first_line = file.readline().strip()\n",
    "\n",
    "    if first_line.startswith('network_countrycode'):\n",
    "        return 'CSV'\n",
    "    elif first_line.startswith('<!DOCTYPE html'):\n",
    "        return 'HTML'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Example usage\n",
    "file_path = 'data_prova/22-05-2023_16_32_14/ES_PM25.csv'  # Replace with the actual path to your file\n",
    "file_type = check_file_content(file_path)\n",
    "print(\"File content type:\", file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs is not defined or is an empty list\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "if dfs:\n",
    "    print('ok')\n",
    "else:\n",
    "    print(\"dfs is not defined or is an empty list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI_CO.csv not exist 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_out = 'data_prova'\n",
    "dir = '22-05-2023_17_36_41'\n",
    "fileName = \"FI_CO.csv\" \n",
    "with open(os.path.join(folder_out, dir, fileName), 'r') as file:\n",
    "    first_line = file.readline().strip()\n",
    "\n",
    "if first_line.startswith('network_countrycode'):\n",
    "    print(fileName,'exist')\n",
    "elif first_line.startswith('<!DOCTYPE html'):\n",
    "    print(fileName,'not exist 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_code</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_altitude</th>\n",
       "      <th>network_countrycode</th>\n",
       "      <th>pollutant</th>\n",
       "      <th>value_datetime_begin</th>\n",
       "      <th>value_datetime_end</th>\n",
       "      <th>value_datetime_updated</th>\n",
       "      <th>value_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT0SON1</td>\n",
       "      <td>Sonnblick</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>AT</td>\n",
       "      <td>CO</td>\n",
       "      <td>2023-05-20 16:00:00+01:00</td>\n",
       "      <td>2023-05-20 17:00:00+01:00</td>\n",
       "      <td>2023-05-20 17:29:08+01:00</td>\n",
       "      <td>0.134442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT0SON1</td>\n",
       "      <td>Sonnblick</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>AT</td>\n",
       "      <td>CO</td>\n",
       "      <td>2023-05-20 17:00:00+01:00</td>\n",
       "      <td>2023-05-20 18:00:00+01:00</td>\n",
       "      <td>2023-05-20 18:32:01+01:00</td>\n",
       "      <td>0.132114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT0SON1</td>\n",
       "      <td>Sonnblick</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>AT</td>\n",
       "      <td>CO</td>\n",
       "      <td>2023-05-20 18:00:00+01:00</td>\n",
       "      <td>2023-05-20 19:00:00+01:00</td>\n",
       "      <td>2023-05-20 19:29:01+01:00</td>\n",
       "      <td>0.134442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT0SON1</td>\n",
       "      <td>Sonnblick</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>AT</td>\n",
       "      <td>CO</td>\n",
       "      <td>2023-05-20 20:00:00+01:00</td>\n",
       "      <td>2023-05-20 21:00:00+01:00</td>\n",
       "      <td>+01:00</td>\n",
       "      <td>0.127458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT0SON1</td>\n",
       "      <td>Sonnblick</td>\n",
       "      <td>3106.0</td>\n",
       "      <td>AT</td>\n",
       "      <td>CO</td>\n",
       "      <td>2023-05-20 21:00:00+01:00</td>\n",
       "      <td>2023-05-20 22:00:00+01:00</td>\n",
       "      <td>+01:00</td>\n",
       "      <td>0.132114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_code station_name  station_altitude network_countrycode pollutant  \\\n",
       "0      AT0SON1    Sonnblick            3106.0                  AT        CO   \n",
       "1      AT0SON1    Sonnblick            3106.0                  AT        CO   \n",
       "2      AT0SON1    Sonnblick            3106.0                  AT        CO   \n",
       "3      AT0SON1    Sonnblick            3106.0                  AT        CO   \n",
       "4      AT0SON1    Sonnblick            3106.0                  AT        CO   \n",
       "\n",
       "        value_datetime_begin         value_datetime_end  \\\n",
       "0  2023-05-20 16:00:00+01:00  2023-05-20 17:00:00+01:00   \n",
       "1  2023-05-20 17:00:00+01:00  2023-05-20 18:00:00+01:00   \n",
       "2  2023-05-20 18:00:00+01:00  2023-05-20 19:00:00+01:00   \n",
       "3  2023-05-20 20:00:00+01:00  2023-05-20 21:00:00+01:00   \n",
       "4  2023-05-20 21:00:00+01:00  2023-05-20 22:00:00+01:00   \n",
       "\n",
       "      value_datetime_updated  value_numeric  \n",
       "0  2023-05-20 17:29:08+01:00       0.134442  \n",
       "1  2023-05-20 18:32:01+01:00       0.132114  \n",
       "2  2023-05-20 19:29:01+01:00       0.134442  \n",
       "3                     +01:00       0.127458  \n",
       "4                     +01:00       0.132114  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "full_path = 'data_prova/se4g_pollution_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(full_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-20 16:00:00+01:00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datetime.fromisoformat(df['value_datetime_begin'][0])\n",
    "datetime.fromisoformat(df['value_datetime_begin'][:]) < datetime.fromisoformat(new_df['value_datetime_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date is before the other_date.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert the string to a datetime object\n",
    "date_str = '2023-05-20 20:00:00+01:00'\n",
    "date_obj = datetime.fromisoformat(date_str)\n",
    "\n",
    "# Compare with another datetime object\n",
    "other_date = datetime.fromisoformat('2023-05-20 20:10:00+01:00')  # Example: May 21, 2023, 10:30 AM\n",
    "\n",
    "if date_obj < other_date:\n",
    "    print(\"The date is before the other_date.\")\n",
    "elif date_obj > other_date:\n",
    "    print(\"The date is after the other_date.\")\n",
    "else:\n",
    "    print(\"The date is equal to the other_date.\")"
=======
    "# Update the final dataset\n",
    "update_dataset(df, folder_out = 'data_prova')"
>>>>>>> 38308f4d9f8e3a26b5684196693af5b92cfb7949
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se4g",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
