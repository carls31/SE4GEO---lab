{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public IP address is: 93.70.3.252\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_public_ip():\n",
    "    response = requests.get('https://api.ipify.org')\n",
    "    return response.text\n",
    "\n",
    "public_ip = get_public_ip()\n",
    "print(\"Your public IP address is:\", public_ip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir:  c:\\Users\\Utente\\Documents\\GitHub\\SE4GEO-Lab\\db_helper\n",
      "Current working directory: c:\\Users\\Utente\\Documents\\GitHub\\SE4GEO-Lab\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory\n",
    "current_dir = os.getcwd()\n",
    "print('current_dir: ',current_dir)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected with  localhost\n"
     ]
    }
   ],
   "source": [
    "ip = '192.168.30.19'\n",
    "ip = 'localhost'\n",
    "file = 'bin.txt'\n",
    "with open('code/'+file, 'r') as f:\n",
    "    engine = create_engine('postgresql://postgres:'+f.read()+'@'+ip+':5432/se4g') \n",
    "con = engine.connect()\n",
    "print('connected with ',ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected with  localhost\n"
     ]
    }
   ],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host = ip,\n",
    "    database = \"se4g\",\n",
    "    user = \"postgres\",\n",
    "    password = \"carIs3198\"\n",
    ")\n",
    "print('connected with ',ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(table_name, df, conn, df_columns = ['station_code', \n",
    "                                                    'station_name', \n",
    "                                                    'station_altitude', \n",
    "                                                    'network_countrycode', \n",
    "                                                    'pollutant', \n",
    "                                                    'value_datetime_begin',\n",
    "                                                    'value_datetime_end',\n",
    "                                                    'value_datetime_updated',\n",
    "                                                    'value_numeric']):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Iterate over the DataFrame rows and insert data row by row\n",
    "    for _, row in df.iterrows():\n",
    "        # Generate the SQL INSERT statement\n",
    "        insert_statement = f\"INSERT INTO {table_name} ({', '.join(df_columns)}) VALUES ({', '.join(['%s'] * len(df_columns))})\"\n",
    "        values = tuple(row[col] for col in df_columns)\n",
    "\n",
    "        # Execute the INSERT statement\n",
    "        cur.execute(insert_statement, values)\n",
    "\n",
    "    \n",
    "    # Commit the changes and close the cursor \n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the final dataset\n",
    "def update_DB(new_df, connection, engine, table_name='se4g_pollution'):\n",
    "\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "\n",
    "    df['value_datetime_begin'] = pd.to_datetime(df['value_datetime_begin'])\n",
    "    new_df['value_datetime_begin'] = pd.to_datetime(new_df['value_datetime_begin'])\n",
    "    #new_df.loc[:, 'value_datetime_begin'] = pd.to_datetime(new_df['value_datetime_begin'])\n",
    "\n",
    "    # Filter rows from new_df based on the datetime\n",
    "    filtered_rows = new_df[new_df['value_datetime_begin'] > df['value_datetime_begin'].max()]\n",
    "\n",
    "    if filtered_rows.empty:\n",
    "        print(\"Nothing to update inside database \",table_name)\n",
    "\n",
    "    elif not filtered_rows.empty:\n",
    "\n",
    "        # Update the dataset by adding the filtered rows\n",
    "        #filtered_rows.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "\n",
    "        # Update the dataset by adding the filtered rows\n",
    "        insert_data(table_name, filtered_rows, connection)\n",
    "        print(\"Database \",table_name,\" updated successfully\")\n",
    "\n",
    "        return filtered_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dashboard_DB(new_rows, connection, table_name='se4g_dashboard'):\n",
    "    columns = ['pollutant', 'country', 'month_day', 'value_numeric_mean', 'value_datetime_begin']\n",
    "    country = {'AD': 'Andorra', 'SE': 'Sweden', 'DE': 'Germany', 'CY': 'Undefined', 'BE': 'Belgium',\n",
    "               'FI': 'Finland', 'ES': 'Spain', 'CZ': 'Czech Republic', 'BG': 'Bulgaria', 'BA': 'Bosnia and Herzegovina',\n",
    "               'EE': 'Estonia', 'CH': 'Switzerland', 'AT': 'Austria', 'DK': 'Denmark'}\n",
    "\n",
    "    # Convert 'value_datetime_end' to datetime objects\n",
    "    datetime_objects = new_rows['value_datetime_end'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S%z'))\n",
    "    new_rows['month_day'] = datetime_objects.dt.strftime('%m%d')\n",
    "    new_rows['value_datetime_begin'] = pd.to_datetime(new_rows['value_datetime_begin']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Compute daily mean of 'value_numeric' for each 'pollutant' and 'network_countrycode'\n",
    "    daily_mean = new_rows.groupby(['pollutant', 'network_countrycode', 'month_day'])['value_numeric'].mean().reset_index()\n",
    "\n",
    "    # Merge the daily mean back to the original dataframe\n",
    "    new_rows = new_rows.merge(daily_mean, on=['pollutant', 'network_countrycode', 'month_day'], suffixes=('', '_mean'))\n",
    "\n",
    "    new_rows['country'] = new_rows['network_countrycode'].map(country)\n",
    "    new_rows = new_rows[columns].copy()\n",
    "    new_rows = new_rows.drop_duplicates().reset_index(drop=True)\n",
    "    new_rows = new_rows.sort_values('month_day')\n",
    "\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql_query(query, connection)\n",
    "\n",
    "    df_value_datetime_begin = pd.to_datetime(df['value_datetime_begin']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print(\"New rows: \\n\",new_rows)\n",
    "    print(\"Max value_datetime_begin in new_rows: \\n\",new_rows['value_datetime_begin'].max())\n",
    "    print(\"Max value_datetime_begin in df: \\n\",df['value_datetime_begin'].max())\n",
    "    \n",
    "    filtered_rows = new_rows[new_rows['value_datetime_begin'] > df_value_datetime_begin.max()]\n",
    "\n",
    "    if filtered_rows.empty:\n",
    "        print(\"Nothing to update inside database\", table_name)\n",
    "    else:\n",
    "        insert_data(table_name, filtered_rows, connection, df_columns=columns)\n",
    "        print(\"Database\", table_name, \"updated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from se4g_helper import download_request, build_dataframe, update_dataset, update_dashboard_dataset\n",
    "# Download and get the dataframe file name\n",
    "dir = download_request(folder_out = 'data_prova')\n",
    "\n",
    "# Build the dataframe with the required structure\n",
    "df = build_dataframe(dir, folder_out = 'data_prova')\n",
    "\n",
    "# Update the main dataset & the dashboard dataset \n",
    "new_df = update_dataset(df, folder_out = 'data_prova')\n",
    "update_dashboard_dataset(df, folder_out = 'data_prova')\n",
    "\n",
    "# Update DBs \n",
    "filtered_rows = update_DB(new_df, con, table_name = 'se4g_pollution')\n",
    "update_dashboard_DB(filtered_rows, con, table_name = 'se4g_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "query = \"SELECT * FROM se4g_pollution\"\n",
    "\n",
    "# Execute the query\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all the results\n",
    "results = cur.fetchall()\n",
    "\n",
    "# Get the column names from the cursor description\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Create a DataFrame from the results and column names\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "\n",
    "# Print the DataFrame\n",
    "#print(df[df['value_datetime_begin']==df['value_datetime_begin'].max()])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-22 06:00:00\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.to_datetime('2023-05-22 06:00:00+01:00')\n",
    "data2 = pd.to_datetime('2023-06-02 01:00:00+00')\n",
    "data1_str = data1.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(data1_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-05-22 06:00:00+0100', tz='pytz.FixedOffset(60)')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-06-02 01:00:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pollutant    value_datetime_begin         value_datetime_end\n",
      "0        SO2  2023-05-29 07:00:00+00  2023-05-29 09:00:00+01:00\n",
      "1        SO2  2023-05-29 08:00:00+00  2023-05-29 10:00:00+01:00\n",
      "2        SO2  2023-05-29 09:00:00+00  2023-05-29 11:00:00+01:00\n",
      "3         NO  2023-05-29 07:00:00+00  2023-05-29 09:00:00+01:00\n",
      "4         NO  2023-05-29 08:00:00+00  2023-05-29 10:00:00+01:00\n",
      "5         NO  2023-05-29 09:00:00+00  2023-05-29 11:00:00+01:00\n",
      "6        NO2  2023-05-29 07:00:00+00  2023-05-29 09:00:00+01:00\n",
      "7        NO2  2023-05-29 08:00:00+00  2023-05-29 10:00:00+01:00\n",
      "8        NO2  2023-05-29 09:00:00+00  2023-05-29 11:00:00+01:00\n",
      "9         CO  2023-05-29 07:00:00+00  2023-05-29 09:00:00+01:00\n",
      "10        CO  2023-05-29 08:00:00+00  2023-05-29 10:00:00+01:00\n",
      "11        CO  2023-05-29 09:00:00+00  2023-05-29 11:00:00+01:00\n",
      "12      PM10  2023-05-29 07:00:00+00  2023-05-29 09:00:00+01:00\n",
      "13      PM10  2023-05-29 08:00:00+00  2023-05-29 10:00:00+01:00\n",
      "14      PM10  2023-05-29 09:00:00+00  2023-05-29 11:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "# Create a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Specify the datetime value\n",
    "datetime_value_begin = '2023-05-29 08:00:00+01:00'\n",
    "datetime_value_end = '2023-05-29 12:00:00+01:00'\n",
    "\n",
    "# Specify the network countrycode\n",
    "network_countrycode = 'AD'\n",
    "\n",
    "# Construct the SQL query with the conditions\n",
    "query = f\"SELECT pollutant, value_datetime_begin, value_datetime_end FROM se4g_pollution WHERE value_datetime_end > %s AND value_datetime_end < %s AND network_countrycode = %s\"\n",
    "\n",
    "# Execute the query\n",
    "cur.execute(query, (datetime_value_begin, datetime_value_end, network_countrycode))\n",
    "\n",
    "# Fetch all rows from the result set\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Get the column names\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Create a pandas DataFrame from the rows and columns\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Close the cursor\n",
    "cur.close()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se4g",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
